{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started with PySpark\n",
    "#### Create spark context and other python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext \n",
    "from StringIO import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set('spark.executor.instances', 2)\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few preliminaries\n",
    "\n",
    "#### lambda expressions\n",
    "It is helpful to know lambda expressions and about unpacking arguments arguments for them.\n",
    "\n",
    "<code>lambda</code> is a statement for an expression that can have a closure.  It's like a function or other callable, but in one line without a return statement.\n",
    "\n",
    "<code>lambda</code> is useful in pyspark because there is a need for a lot of simple callables.\n",
    "\n",
    "#### mapping / reducing\n",
    "In pyspark and other parallel computing frameworks, there is a concept of 'mapping'.\n",
    "\n",
    "E.g. mapping across a list of tuples and summing the second element of each tuple\n",
    "```python\n",
    "x = [(1, 2), (2, 3)]\n",
    "\n",
    "sum(map(lambda x:x[1], x)) # answer is 5 in python\n",
    "# or\n",
    "sc.parallelize(x).map(lambda x:x[1]).sum() # answer is 5 in pyspark\n",
    "```\n",
    "In mapping and reducing with pyspark it can be helpful to unpack the arguments to a lambda used in a mapper.\n",
    "\n",
    "```python\n",
    "sc.parallelize(x).map(lambda (key, value): value).sum() # 5\n",
    "```\n",
    "(x was mapped as a Resilient Distributed Dataset, the key-values were split and the values were summed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sc.parallelize: create an RDD from a python iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def generate_rands(length=100):\n",
    "    for idx in range(length):\n",
    "        yield (random.randint(0,1), random.randint(0,1))\n",
    "rand_rdd = sc.parallelize(generate_rands())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rand_rdd is represented now in parallel java processes for use with pyspark map reduce\n",
    "If we want to get the RDD back to python, we can call .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But a better way to do it is to <code>take</code> a few elements rather than the full RDD\n",
    "This avoids pulling the entire RDD into the main python process, thereby breaking parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sc.binaryFiles: read binary HDFS files into an RDD, matching a wildcard pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_files = sc.binaryFiles('/img/*')  # wildcard hadoop distributed file system name\n",
    "img_files.take(1) # Don't call .collect() on this one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sc.pickleFile: read results typically from pyspark that were output with saveAsPickleFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_measurements = sc.pickleFile('hdfs:///t1/map_each_image/measures')\n",
    "image_measurements.take(1)  # Don't call .collect() on this one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### .map applys a function to each element of the RDD\n",
    "This functions maps x and y random numbers and calculates their squares and product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products = rand_rdd.map(lambda (x, y): (x**2, y**2, x*y))\n",
    "products.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "####.reduce applys an aggregation to an RDD\n",
    ".reduce takes a callable which is called with two elements of the RDD (repeatedly)\n",
    "\n",
    "Here is the covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products.map(lambda (x_square, y_square, xy): (xy)).reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going through some map reduce ideas with the image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping the 'load_image' function to filenames will load each filename's image into a Resilient Distributed Dataset (RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_image(image):\n",
    "    \"\"\"Load one image, where image = (key, blob)\"\"\"\n",
    "    from StringIO import StringIO\n",
    "    from PIL import Image\n",
    "    img = Image.open(StringIO(image[1]))\n",
    "    return  image[0], np.asarray(img, dtype=np.uint8)\n",
    "\n",
    "img_files = sc.binaryFiles('/img/malestaff*')  # wildcard hadoop distributed file system name\n",
    "img_mapped = sc.map(load_image, img_files)\n",
    "img_mapped.cache() # cache this RDD for later use (only a performance helper)\n",
    "img_mapped.take(1) # take the 1st one and repr it to see what it looks like\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As shown above, img_mapped is a iterable of tuples (filename, image).  We can in turn map the RDD of images to an analysis function.  Here we are calculating the percentiles of one band of color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "band = 0 # red\n",
    "red_percentiles = img_mapped.map(\n",
    "                        lambda (fname, img): np.percentile(img[:,:,band], (5, 25, 50, 75, 95))\n",
    "                    )\n",
    "red_percentiles.take(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With spark, we can also reduce an RDD.  This will take an average of the red percentiles by summing and dividing by the count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avgs = red_percentiles.reduce(\n",
    "                            lambda a, b: a + b\n",
    "                        ) / red_percentiles.count()\n",
    "print(avgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Example of groupby operations, with a random grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_groups = img_mapped.map(\n",
    "                lambda (fname, img): (random.randint(0,2), (fname, img))\n",
    "            ).groupByKey(\n",
    "            )\n",
    "\n",
    "print(\"Grouped\", rand_groups.take(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating a mean color within the groups.  Collect brings the results to python as list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_groups.map(\n",
    "        lambda (rnd, results): (rnd, np.mean((np.mean(img[:,:,band]) for fname, img in results)))\n",
    "    ).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using some of the outputs of image-analyzer, the measurements taken on each raw image.  We have saved them as a pickleFile RDD, so we can load the RDD with pickleFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = yaml.load(open('config.yaml').read())\n",
    "output_hdfs = 'hdfs:///t1/map_each_image/measures_2'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### map the measurements function to the images, saving at output_hdfs path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_file_spec = '/imgs/ma*'\n",
    "map_each_image(sc, config, input_file_spec, output_hdfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### later load the measures from hdfs using unpickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "measures = sc.pickleFile(output_hdfs)\n",
    "print(measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting an RDD, to see what it looks like, we have to call <code>take</code> to see element(s) of the RDD and we can do type inspection to figure out what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = measures.take(1)\n",
    "print('example is of type', type(example))\n",
    "key, value = example\n",
    "print('value is of type', type(value))\n",
    "print('and has keys of', tuple(value.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the image analyzer code we know that the 'histo' key is the 3 band histogram flattened into one array, so we can check its size and know where the median colors are.  For example, if it is 21 elements long, then the first 7 are the red histogram and the middle of those 7 elements is the median red color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_median_from_histo(histo, band):\n",
    "    lh = len(histo)\n",
    "    return histo[lh // 2 + 1 + band * lh]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As an exercise, sort the images by median red color.  (band = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "measures.sortBy(lambda (key, value): get_median_from_histo(value['histo'], 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Make a function to print out images in the same order they are found in <code>measures</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imshow_measures(measures):\n",
    "    collected_imgs = meaures.map(lambda x:load_image(x[0])).collect()\n",
    "    map(lambda key, value: plt.imshow(value), collected_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imshow_measures(measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sort by median in the green band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "measures.sortBy(lambda (key, value): get_median_from_histo(value['histo']), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### show the images again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imshow_measures(measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joins in PySpark work on RDDs of tuples and equality on the first item in the tuples is used for joining\n",
    "\n",
    "Here is an example with a 2 lists of tuples that are spark RDDs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_a = sc.parallelize([(1, 2), (2, 3), (3, 5)])\n",
    "list_b = sc.parallelize([(1, 3), (2, 5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_a.join(list_b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_b.join(list_a).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_a.fullRightJoin(list_b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_a.fullLeftJoin(list_b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joins with the image data\n",
    "In the example images we have /img/ which includes the original and /fuzzy/ which is a fuzzy version of those.  We can map the measurements of fuzzy and original images and join on ward cluster hashes.  In each image's measures dictionary, the ward clusters are saved in a list.  We can use <code>flatMap</code> to flatten that list and create an RDD with tuples of (one_ward_cluster_hash, image_file_name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "candidates = sc.pickleFile('hdfs:///c1/map_each_image/measures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten_ward(rdd):\n",
    "    return rdd.flatMap(lambda (key, value): [(wc, key) for wc in value['ward']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "candidates_flat = flatten_ward(candidates)\n",
    "originals_flat = flatten_ward(measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined = candidates_flat.join(originals_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
