{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with PySpark\n",
    "#### Create spark context and other python imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function, division\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext \n",
    "from StringIO import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set('spark.executor.instances', 2)\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few preliminaries\n",
    "\n",
    "#### <code>lambda</code> expressions\n",
    "It is helpful to know lambda expressions and about unpacking arguments arguments for them.\n",
    "\n",
    "<code>lambda</code> is a statement for an expression that can have a closure.  It's like a function or other callable, but in one line without a return statement.\n",
    "\n",
    "<code>lambda</code> is useful in pyspark because there is a need for a lot of simple callables.\n",
    "\n",
    "<code>c=lambda (key,(x,y)): (key, x * y)</code> \n",
    "\n",
    "makes c a callable, that can be used like this:\n",
    "\n",
    "<code>c(('key1', (3, 4)))</code> \n",
    "\n",
    "to return \n",
    "<code>('key1', 12)</code> \n",
    "\n",
    "#### mapping / reducing\n",
    "In pyspark and other parallel computing frameworks, there is a concept of 'mapping'.\n",
    "\n",
    "E.g. mapping across a list of tuples and summing the second element of each tuple\n",
    "```python\n",
    "x = [(1, 2), (2, 3)]\n",
    "\n",
    "sum(map(lambda x:x[1], x)) # answer is 5 in python\n",
    "# or\n",
    "sc.parallelize(x).map(lambda x:x[1]).sum() # answer is 5 in pyspark\n",
    "```\n",
    "In mapping and reducing with pyspark it can be helpful to unpack the arguments to a lambda used in a mapper.\n",
    "\n",
    "```python\n",
    "sc.parallelize(x).map(lambda (key, value): value).sum() # 5\n",
    "```\n",
    "(x was mapped as a Resilient Distributed Dataset, the key-values were split and the values were summed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common pyspark usage patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <code>sc.parallelize</code>: create an RDD from a python iterable\n",
    "\n",
    "<code>sc.parallelize</code> can be called on a list, tuple, generator, or other iterator.  Generators are useful because the full set of elements of the RDD never has to be held in memory in the python main process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def generate_rands(length=100):\n",
    "    for idx in range(length):\n",
    "        yield (random.uniform(0,1), random.uniform(0,1))\n",
    "rand_rdd = sc.parallelize(generate_rands())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have just created rand_rdd in parallel java processes for use with pyspark map reduce\n",
    "If we want to get the RDD back to python, we can call <code>.collect()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8233133911430207, 0.23390036322252228),\n",
       " (0.5864887904722843, 0.7247920583468935),\n",
       " (0.868808963085547, 0.38927505714571997),\n",
       " (0.7997240345772992, 0.8648685339908081),\n",
       " (0.7887573866879661, 0.2956290317157453),\n",
       " (0.3067619246071419, 0.34909126160939163),\n",
       " (0.4059335260023892, 0.38823106121150397),\n",
       " (0.33601159510207335, 0.4990349540137352),\n",
       " (0.18695004090526468, 0.43572846130587106),\n",
       " (0.13497377912873232, 0.5993620897859301)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "as_list_again = rand_rdd.collect()\n",
    "as_list_again[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But a better way to do it is to <code>take</code> a few elements rather than the full RDD\n",
    "This avoids pulling the entire RDD into the main python process, which breaks parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8233133911430207, 0.23390036322252228),\n",
       " (0.5864887904722843, 0.7247920583468935),\n",
       " (0.868808963085547, 0.38927505714571997),\n",
       " (0.7997240345772992, 0.8648685339908081),\n",
       " (0.7887573866879661, 0.2956290317157453)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <code>sc.binaryFiles</code>: read binary HDFS files into an RDD, matching a wildcard pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'hdfs://ip-10-111-177-131:9000/imgs/malestaffanonymanonym.11.jpg',\n",
      " '\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x08\\x06\\x06\\x07\\x06\\x05\\x08\\x07\\x07\\x07\\t\\t\\x08\\n\\x0c\\x14\\r\\x0c\\x0b\\x0b\\x0c\\x19\\x12\\x13\\x0f\\x14\\x1d\\x1a\\x1f\\x1e\\x1d\\x1a\\x1c\\x1c $.\\' \",#\\x1c\\x1c(7),01444\\x1f\\'9=82<.342\\xff\\xdb\\x00C\\x01\\t\\t\\t\\x0c\\x0b\\x0c')\n"
     ]
    }
   ],
   "source": [
    "img_files = sc.binaryFiles('/imgs/malestaff*')  # wildcard hadoop distributed file system name\n",
    "key, img = img_files.take(1)[0] # Don't call .collect() on this one!\n",
    "pprint((key, img[:100])) # abbreviated the image blob to 100 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <code>sc.pickleFile</code>: read results typically from pyspark that were output with <code>saveAsPickleFile</code>\n",
    "\n",
    "In pyspark, an RDD may be persisted to the hadoop distributed file system (HDFS) with\n",
    "\n",
    "<code>x.saveAsPickleFile(some_path)</code> where <code>x</code> is the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'hdfs://ip-10-111-177-131:9000/imgs/femaleasammaasamma.2.jpg',\n",
       "  {'cen': array([ 0.0980597 ,  0.10067332,  0.07502033,  0.4832288 ,  0.74329001,\n",
       "           0.3068403 ,  0.39556718,  0.33680019,  0.34318587,  0.69543546,\n",
       "           0.5883016 ,  0.61652535,  0.97057652,  0.86680192,  0.84002519,\n",
       "           0.26330549,  0.2297267 ,  0.24091083,  0.89458674,  0.70459908,\n",
       "           0.66731501,  0.28936416,  0.39460137,  0.14140551,  0.04144318,\n",
       "           0.04623584,  0.02837553,  0.40374291,  0.58303112,  0.23085584,\n",
       "           0.53872192,  0.4576503 ,  0.44566652,  0.16590469,  0.16399746,\n",
       "           0.13009502], dtype=float32),\n",
       "   'histo': array([ 0.02744549,  0.03814275,  0.05321374,  0.16956136,  0.45035073,\n",
       "           0.50272733,  0.75232422,  0.0308008 ,  0.04285328,  0.05870309,\n",
       "           0.17326318,  0.51095122,  0.69764537,  0.76385945,  0.01479674,\n",
       "           0.02571036,  0.03825854,  0.13062458,  0.30416244,  0.36708713,\n",
       "           0.64770496], dtype=float32),\n",
       "   'id': u'hdfs://ip-10-111-177-131:9000/imgs/femaleasammaasamma.2.jpg',\n",
       "   'meta': {'app_APP0': 'JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00',\n",
       "    'format': 'JPEG',\n",
       "    'format_description': 'JPEG (ISO 10918)',\n",
       "    'info_jfif': 257,\n",
       "    'info_jfif_density': (1, 1),\n",
       "    'info_jfif_unit': 0,\n",
       "    'info_jfif_version': (1, 1),\n",
       "    'is_full': True},\n",
       "   'pca_fac': array([-0.60659438, -0.64272549, -0.46791793,  0.28722133, -0.72598572,\n",
       "           0.6248589 , -0.74131448,  0.24463989,  0.62498333]),\n",
       "   'pca_var': array([ 0.15888125,  0.01093299,  0.00118719]),\n",
       "   'phash': [4590452597640986239,\n",
       "    5705755325505827644,\n",
       "    4316219442648471965,\n",
       "    -4779138984269271173,\n",
       "    67322450502461386,\n",
       "    -2673027970935517212,\n",
       "    1043306428788482392],\n",
       "   'ward': (-3831614152369362579,\n",
       "    -3831614152369362579,\n",
       "    7497007922500697532,\n",
       "    -9001240377361059183,\n",
       "    -8238206338916862869,\n",
       "    281200309521801431,\n",
       "    -7827924191438386758,\n",
       "    4232277551429531916)})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_measurements = sc.pickleFile('hdfs:///t1/map_each_image/measures')\n",
    "image_measurements.take(1)  # Don't call .collect() on this one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### <code>.map</code> applys a function to each element of the RDD\n",
    "This functions maps x and y random numbers and calculates their squares and product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.6778449400354206, 0.054709379915627854, 0.1925733012343191),\n",
       " (0.343969101349643, 0.5253235278427267, 0.4250824176437869),\n",
       " (0.7548290143377834, 0.15153507011580356, 0.33820565875384),\n",
       " (0.6395585314805933, 0.7479975810874095, 0.6916561533820831),\n",
       " (0.6221382150548297, 0.08739652439318914, 0.2331795824852051)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products = rand_rdd.map(lambda (x, y): (x**2, y**2, x*y))\n",
    "products.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### <code>.reduce</code> applys an aggregation to an RDD\n",
    "<code>.reduce</code> takes a callable which is called with two elements of the RDD (repeatedly).  The function must be idempotent, that is it must give the same answer if called more than once on the same inputs.  Summing is an example of an idempotent operation.\n",
    "\n",
    "Here we calculate the correlation coefficient between the two columns by summing <code class=\"python\">x\\*\\*2</code>, <code class=\"python\">y\\*\\*2</code> and <code>x * y</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.022629099386862826"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_variance, y_variance, covar = products.reduce(lambda a, b: [ai + bi for ai, bi in zip(a,b)]) \n",
    "# in the above line a and b are both tuples 3 long (x square, y square, x * y)\n",
    "correlation = covar / (y_variance * x_variance)\n",
    "correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <code>.flatMap</code> works like <code>.map</code> but <code>.flatMap</code> returns a list\n",
    "\n",
    "It is useful for making more elements out of an element of an RDD, like this word count example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words RDD\n",
      "[('useful', 1),\n",
      " ('for', 1),\n",
      " ('RDDs', 1),\n",
      " ('RDDs', 1),\n",
      " ('are', 1),\n",
      " ('useful', 1),\n",
      " ('flatMap', 1),\n",
      " ('is', 1),\n",
      " ('useful', 1),\n",
      " ('map/reduce', 1),\n",
      " ('is', 1),\n",
      " ('also', 1),\n",
      " ('useful', 1)]\n",
      "Word counts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('useful', 4),\n",
       " ('RDDs', 2),\n",
       " ('is', 2),\n",
       " ('map/reduce', 1),\n",
       " ('are', 1),\n",
       " ('also', 1),\n",
       " ('flatMap', 1),\n",
       " ('for', 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = sc.parallelize(['useful for RDDs', \n",
    "                            'RDDs are useful', \n",
    "                            'flatMap is useful', \n",
    "                            'map/reduce is also useful'])\n",
    "words = documents.flatMap(\n",
    "    lambda x: [(word, 1) for word in x.split()]  #(from documents rdd to words rdd with 1 for counting)\n",
    ")\n",
    "print('words RDD')\n",
    "pprint(words.collect())\n",
    "print('Word counts')\n",
    "words.reduceByKey(\n",
    "    lambda a, b: a + b  # the word is not passed in, only the count which may be 1 or more when called\n",
    ").sortBy(\n",
    "    lambda (word, count): -count   # sort by negative count for descending\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting an RDD, to see what it looks like, we have to call <code>take</code> to see elements of the RDD and we can do type inspection to figure out what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example is of type <type 'str'>\n"
     ]
    }
   ],
   "source": [
    "example = documents.take(1)[0] # take gives a list and we want 1st element\n",
    "print('example is of type', type(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <code>.count()</code> gives the number of elements in an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going through some map reduce ideas with the image files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping the <code>load_image</code> function to filenames will load each filename's image into an RDD\n",
    "The RDD consists of <code>(filename, image_numpy_3d_array)</code> tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs://ip-10-111-177-131:9000/imgs/malestaffanonymanonym.11.jpg has a shape of  (200, 180, 3)\n"
     ]
    }
   ],
   "source": [
    "def load_image(image):\n",
    "    \"\"\"Load one image, where image = (key, blob)\"\"\"\n",
    "    from StringIO import StringIO\n",
    "    from PIL import Image\n",
    "    img = Image.open(StringIO(image[1]))\n",
    "    return  image[0], np.asarray(img, dtype=np.uint8)\n",
    "\n",
    "img_files = sc.binaryFiles('/imgs/malestaff*')  # wildcard hadoop distributed file system name\n",
    "img_mapped = img_files.map(load_image, img_files)\n",
    "key, img_3d = img_mapped.take(1)[0] # Don't call .collect() on this one\n",
    "print(key, \"has a shape of \", img_3d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At this point <code>img_mapped</code> is a iterable of tuples (filename, image).  We can in turn map and reduce this RDD to create other RDDs.  Here we are calculating the percentiles in each color band for each image and printing out the first 5 images' percentils as a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentiles for red\n",
      "[array([  31.,   85.,  114.,  136.,  254.]),\n",
      " array([  31. ,   84. ,  113.5,  136. ,  253. ]),\n",
      " array([  10.,   36.,   71.,   78.,  183.]),\n",
      " array([  10.,   37.,   70.,   78.,  184.]),\n",
      " array([  13.,   33.,   58.,   64.,  115.])]\n",
      "Percentiles for green\n",
      "[array([  27.,   75.,  133.,  180.,  203.]),\n",
      " array([  27.,   73.,  134.,  181.,  202.]),\n",
      " array([  12.,   34.,   84.,  109.,  132.]),\n",
      " array([  11.,   34.,   82.,  108.,  131.]),\n",
      " array([  13.,   32.,   61.,   95.,  104.])]\n",
      "Percentiles for blue\n",
      "[array([  20.,   44.,   76.,  112.,  209.]),\n",
      " array([  20.,   45.,   79.,  110.,  209.]),\n",
      " array([   7.,   26.,   42.,   55.,  129.]),\n",
      " array([   8.,   27.,   42.,   57.,  132.]),\n",
      " array([  11.,   35.,   53.,   66.,  109.])]\n"
     ]
    }
   ],
   "source": [
    "labels_bands = zip(('red','green','blue'), range(3))\n",
    "for label, band in labels_bands:\n",
    "    percentiles = img_mapped.map(\n",
    "                        lambda (fname, img): np.percentile(img[:,:,band], (5, 25, 50, 75, 95))\n",
    "                    )\n",
    "    print('Percentiles for', label)\n",
    "    pprint(percentiles.take(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With spark, we can also reduce an RDD.  This will take an average of the red percentiles by summing and dividing by the count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  16.03125     37.1796875   55.65625     79.8125     153.09375  ]\n"
     ]
    }
   ],
   "source": [
    "avgs = percentiles.reduce(\n",
    "    lambda a, b: a + b  # adding numpy percentiles arrays a and b from 2 images\n",
    ") / percentiles.count() # diving by the number of images\n",
    "print(avgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Example of groupby operations, with grouping by mean color bins\n",
    "Below <code>int(np.mean(img[:,:, band]) // 5) * 5</code> creates an integer key that is the pixel 0 to 255 value floored to the nearest 5.  The first map emits this key with the filename.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping by red\n",
      "[(45,\n",
      "  [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmartinmartin.3.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmartinmartin.6.jpg']),\n",
      " (50, [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffdorajdoraj.3.jpg']),\n",
      " (55, [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffdorajdoraj.13.jpg']),\n",
      " (60,\n",
      "  [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffieorfieorf.2.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffieorfieorf.9.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffrobinrobin.10.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffrobinrobin.4.jpg']),\n",
      " (65,\n",
      "  [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffcwangcwang.12.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffcwangcwang.5.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffvoudcxvoudcx.17.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffvoudcxvoudcx.7.jpg']),\n",
      " (80,\n",
      "  [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmaccimacci.13.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmaccimacci.20.jpg']),\n",
      " (85,\n",
      "  [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmichaelmichael.16.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmichaelmichael.18.jpg']),\n",
      " (95, [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffsandmsandm.4.jpg']),\n",
      " (100,\n",
      "  [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffhartbhartb.10.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffhartbhartb.12.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffhartbhartb.13.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffobeidnobeidn.14.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffobeidnobeidn.7.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffobeidnobeidn.9.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffspaclspacl.18.jpg']),\n",
      " (110,\n",
      "  [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffhensmhensm.12.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffhensmhensm.15.jpg']),\n",
      " (115,\n",
      "  [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffanonymanonym.11.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffanonymanonym.20.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmoorsmoors.14.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmoorsmoors.3.jpg']),\n",
      " (120, [u'hdfs://ip-10-111-177-131:9000/imgs/malestafffordjfordj.1.jpg']),\n",
      " (135, [u'hdfs://ip-10-111-177-131:9000/imgs/malestafftonytony.4.jpg'])]\n",
      "Grouping by green\n",
      "[(50,\n",
      "  [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmartinmartin.3.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmartinmartin.6.jpg']),\n",
      " (60,\n",
      "  [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffdorajdoraj.13.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffdorajdoraj.3.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffieorfieorf.9.jpg']),\n",
      " (65,\n",
      "  [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffieorfieorf.2.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffrobinrobin.10.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffrobinrobin.4.jpg']),\n",
      " (70,\n",
      "  [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffcwangcwang.5.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffvoudcxvoudcx.17.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffvoudcxvoudcx.7.jpg']),\n",
      " (75,\n",
      "  [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffcwangcwang.12.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmaccimacci.13.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmaccimacci.20.jpg']),\n",
      " (80,\n",
      "  [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmichaelmichael.16.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmichaelmichael.18.jpg']),\n",
      " (90, [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffspaclspacl.18.jpg']),\n",
      " (95, [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffsandmsandm.4.jpg']),\n",
      " (105,\n",
      "  [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffobeidnobeidn.14.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffobeidnobeidn.7.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffobeidnobeidn.9.jpg']),\n",
      " (110,\n",
      "  [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffhartbhartb.10.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffhartbhartb.12.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffhartbhartb.13.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffhensmhensm.12.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffhensmhensm.15.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmoorsmoors.14.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmoorsmoors.3.jpg']),\n",
      " (125,\n",
      "  [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffanonymanonym.11.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffanonymanonym.20.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestafffordjfordj.1.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestafftonytony.4.jpg'])]\n",
      "Grouping by blue\n",
      "[(35,\n",
      "  [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmartinmartin.3.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmartinmartin.6.jpg']),\n",
      " (40,\n",
      "  [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffieorfieorf.2.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffieorfieorf.9.jpg']),\n",
      " (45,\n",
      "  [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffcwangcwang.12.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffcwangcwang.5.jpg']),\n",
      " (50,\n",
      "  [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffdorajdoraj.3.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffvoudcxvoudcx.17.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffvoudcxvoudcx.7.jpg']),\n",
      " (55,\n",
      "  [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffdorajdoraj.13.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmaccimacci.13.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmaccimacci.20.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffrobinrobin.10.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffrobinrobin.4.jpg']),\n",
      " (60, [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffspaclspacl.18.jpg']),\n",
      " (65,\n",
      "  [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmichaelmichael.16.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffsandmsandm.4.jpg']),\n",
      " (70,\n",
      "  [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffhartbhartb.10.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffhartbhartb.12.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffhartbhartb.13.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmichaelmichael.18.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffobeidnobeidn.14.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffobeidnobeidn.7.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffobeidnobeidn.9.jpg']),\n",
      " (75,\n",
      "  [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffhensmhensm.12.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffhensmhensm.15.jpg']),\n",
      " (80,\n",
      "  [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmoorsmoors.14.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmoorsmoors.3.jpg']),\n",
      " (85,\n",
      "  [u'hdfs://ip-10-111-177-131:9000/imgs/malestaffanonymanonym.11.jpg',\n",
      "   u'hdfs://ip-10-111-177-131:9000/imgs/malestaffanonymanonym.20.jpg']),\n",
      " (90, [u'hdfs://ip-10-111-177-131:9000/imgs/malestafffordjfordj.1.jpg']),\n",
      " (95, [u'hdfs://ip-10-111-177-131:9000/imgs/malestafftonytony.4.jpg'])]\n"
     ]
    }
   ],
   "source": [
    "for label, band in labels_bands:\n",
    "    rand_groups = img_mapped.map(\n",
    "                lambda (fname, img): (int(np.mean(img[:,:, band]) // 5) * 5, fname)\n",
    "            ).groupByKey(\n",
    "            ).map(\n",
    "                lambda (group, fnames): (group, list(fnames))\n",
    "            ).sortByKey(\n",
    "            )\n",
    "    print('Grouping by', label)\n",
    "    pprint(rand_groups.collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next few steps load the configuration yaml file for image-analyzer, set an output hdfs location, and apply the map_each_image function to write an RDD of image measurements to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[75] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "from map_each_image import map_each_image\n",
    "config = yaml.load(open('config.yaml').read())\n",
    "config['random_state'] = 42\n",
    "# make a new filename each time for the demo starting with /%s/map_each_image/ % config['test_name']\n",
    "output_hdfs = '/%s/map_each_image/measures_8'  % config['test_name']\n",
    "input_file_spec = '/imgs/malestaff*'\n",
    "measures = map_each_image(sc, config, input_file_spec, output_hdfs)\n",
    "measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### later load the image measurements from HDFS using unpickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'hdfs://ip-10-111-177-131:9000/imgs/malestaffanonymanonym.11.jpg',\n",
       "  {'cen': array([ 0.35465017,  0.29722726,  0.25416034,  0.60304958,  0.53971756,\n",
       "           0.37986243,  0.98485434,  0.81124085,  0.86357927,  0.36931238,\n",
       "           0.47650814,  0.15291676,  0.22837859,  0.19009633,  0.1671942 ,\n",
       "           0.67391962,  0.54674029,  0.57025176,  0.49174181,  0.73113638,\n",
       "           0.32001153,  0.44407496,  0.39712024,  0.44328627,  0.12760539,\n",
       "           0.11378499,  0.08404868,  0.98294377,  0.76325887,  0.68695372,\n",
       "           0.40710312,  0.56513143,  0.23902495,  0.83226156,  0.65122527,\n",
       "           0.6805672 ], dtype=float32),\n",
       "   'histo': array([ 0.12638396,  0.22037019,  0.33601463,  0.44619676,  0.53434545,\n",
       "           0.7464537 ,  0.99226081,  0.11083502,  0.17869972,  0.3017368 ,\n",
       "           0.52392244,  0.70189315,  0.74901539,  0.79198784,  0.07672456,\n",
       "           0.13942909,  0.17422667,  0.29805189,  0.43718517,  0.60197175,\n",
       "           0.80762595], dtype=float32),\n",
       "   'id': u'hdfs://ip-10-111-177-131:9000/imgs/malestaffanonymanonym.11.jpg',\n",
       "   'meta': {'app_APP0': 'JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00',\n",
       "    'format': 'JPEG',\n",
       "    'format_description': 'JPEG (ISO 10918)',\n",
       "    'info_jfif': 257,\n",
       "    'info_jfif_density': (1, 1),\n",
       "    'info_jfif_unit': 0,\n",
       "    'info_jfif_version': (1, 1),\n",
       "    'is_full': True},\n",
       "   'pca_fac': array([-0.6376551 , -0.54587433, -0.54352293,  0.19410426, -0.79667136,\n",
       "           0.57239696, -0.74546596,  0.25949173,  0.61395809]),\n",
       "   'pca_var': array([ 0.13378774,  0.01948898,  0.00360354]),\n",
       "   'phash': [3960697120294166445, -7370409000308857715, -3669256703792274681],\n",
       "   'ward': (-3831614152369362579,\n",
       "    -3831614152369362579,\n",
       "    -8238206338916862869,\n",
       "    3089103728550035081,\n",
       "    6203878385188208019)})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measures = sc.pickleFile(output_hdfs)\n",
    "measures.take(1) # don't call .collect() here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the image analyzer code we know that the <code>'histo'</code> key is the 3 band histogram flattened into one array, so we can check its size and know where the median colors are.  For example, if it is 21 elements long, then the first 7 are the red histogram and the middle of those 7 elements is the median red color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_median_from_histo(histo, band):\n",
    "    lh = len(histo) // 3\n",
    "    return histo[lh // 2 + 1 + band * lh]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As an exercise, sort the images by median color in each band, take the 5 image file names that are lowest in that color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest 5 images in red \n",
      "[u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmartinmartin.3.jpg',\n",
      " u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmartinmartin.6.jpg',\n",
      " u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmartinmartin.3.jpg',\n",
      " u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmartinmartin.6.jpg',\n",
      " u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmartinmartin.3.jpg']\n",
      "Lowest 5 images in green \n",
      "[u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmartinmartin.3.jpg',\n",
      " u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmartinmartin.6.jpg',\n",
      " u'hdfs://ip-10-111-177-131:9000/imgs/malestaffdorajdoraj.13.jpg',\n",
      " u'hdfs://ip-10-111-177-131:9000/imgs/malestaffdorajdoraj.3.jpg',\n",
      " u'hdfs://ip-10-111-177-131:9000/imgs/malestaffdorajdoraj.3.jpg']\n",
      "Lowest 5 images in blue \n",
      "[u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmartinmartin.6.jpg',\n",
      " u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmartinmartin.3.jpg',\n",
      " u'hdfs://ip-10-111-177-131:9000/imgs/malestaffieorfieorf.9.jpg',\n",
      " u'hdfs://ip-10-111-177-131:9000/imgs/malestaffieorfieorf.2.jpg',\n",
      " u'hdfs://ip-10-111-177-131:9000/imgs/malestaffcwangcwang.12.jpg']\n"
     ]
    }
   ],
   "source": [
    "for label, band in labels_bands:\n",
    "    sorted_filenames = measures.sortBy(\n",
    "        lambda (key, value): get_median_from_histo(value['histo'], band)\n",
    "    ).map(\n",
    "        lambda (key, value): key\n",
    "    ).collect()\n",
    "    print('Lowest 5 images in %s ' % label)\n",
    "    pprint(sorted_filenames[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Make a function to print out images in the same order they are found in <code>measures</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joins in PySpark work on RDDs, using equality on the first item in tuples to determine the join\n",
    "\n",
    "Here is are several join examples with a 2 lists of tuples that are spark RDDs from python lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (1, 2)), (3, (3, 4))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_a = sc.parallelize([(1, 1), (3, 3), (5, 10)]) # a has some keys not in b's keys\n",
    "list_b = sc.parallelize([(1, 2), (3, 4), (7, 12)]) # b has some keys not in a's keys\n",
    "list_a.join(list_b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (2, 1)), (3, (4, 3))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_b.join(list_a).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (1, 2)), (3, (3, 4)), (7, (None, 12))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_a.rightOuterJoin(list_b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (2, 1)), (3, (4, 3)), (7, (12, None))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_b.leftOuterJoin(list_a).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (2, 1)), (3, (4, 3)), (5, (None, 10))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_b.rightOuterJoin(list_a).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (1, 2)), (3, (3, 4)), (5, (10, None))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_a.leftOuterJoin(list_b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (1, 2)), (3, (3, 4)), (5, (10, None)), (7, (None, 12))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_a.fullOuterJoin(list_b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joins with the image data\n",
    "In the example images we have <code>hdfs:///imgs/</code> which includes the original and <code>hdfs:///fuzzy/</code> which is a fuzzy version of those.  We can map the measurements of fuzzy and original images and join on ward cluster hashes.  In each image's measures dictionary, the ward clusters are saved in a list, as shown above.  We can use <code>flatMap</code> to flatten that list and create an RDD with tuples of <code>(one_ward_cluster_hash, image_file_name)</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parts = (config['test_name'], config['candidate_batch'])\n",
    "candidates = sc.pickleFile('hdfs:///%s/candidates/%s/measures' % parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten_ward(rdd):\n",
    "    return rdd.flatMap(lambda (key, value): [(wc, key) for wc in value['ward']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-3831614152369362579,\n",
      "  u'hdfs://ip-10-111-177-131:9000/fuzzy/femaleanpageanpage.20.jpg')]\n",
      "[(-3831614152369362579,\n",
      "  u'hdfs://ip-10-111-177-131:9000/imgs/malestaffanonymanonym.11.jpg')]\n"
     ]
    }
   ],
   "source": [
    "candidates_flat = flatten_ward(candidates)\n",
    "originals_flat = flatten_ward(measures)\n",
    "pprint(candidates_flat.take(1))\n",
    "pprint(originals_flat.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joined = candidates_flat.join(originals_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-5265720812001719200,\n",
       "  [(u'hdfs://ip-10-111-177-131:9000/fuzzy/femaleanpageanpage.20.jpg',\n",
       "    u'hdfs://ip-10-111-177-131:9000/imgs/malestaffieorfieorf.9.jpg'),\n",
       "   (u'hdfs://ip-10-111-177-131:9000/fuzzy/femaleanpageanpage.20.jpg',\n",
       "    u'hdfs://ip-10-111-177-131:9000/imgs/malestaffsandmsandm.4.jpg'),\n",
       "   (u'hdfs://ip-10-111-177-131:9000/fuzzy/femaleanpageanpage.20.jpg',\n",
       "    u'hdfs://ip-10-111-177-131:9000/imgs/malestaffvoudcxvoudcx.7.jpg'),\n",
       "   (u'hdfs://ip-10-111-177-131:9000/fuzzy/malecshubbcshubb.10.jpg',\n",
       "    u'hdfs://ip-10-111-177-131:9000/imgs/malestaffieorfieorf.9.jpg'),\n",
       "   (u'hdfs://ip-10-111-177-131:9000/fuzzy/malecshubbcshubb.10.jpg',\n",
       "    u'hdfs://ip-10-111-177-131:9000/imgs/malestaffsandmsandm.4.jpg'),\n",
       "   (u'hdfs://ip-10-111-177-131:9000/fuzzy/malecshubbcshubb.10.jpg',\n",
       "    u'hdfs://ip-10-111-177-131:9000/imgs/malestaffvoudcxvoudcx.7.jpg'),\n",
       "   (u'hdfs://ip-10-111-177-131:9000/fuzzy/malejdbenmjdbenm.20.jpg',\n",
       "    u'hdfs://ip-10-111-177-131:9000/imgs/malestaffieorfieorf.9.jpg'),\n",
       "   (u'hdfs://ip-10-111-177-131:9000/fuzzy/malejdbenmjdbenm.20.jpg',\n",
       "    u'hdfs://ip-10-111-177-131:9000/imgs/malestaffsandmsandm.4.jpg'),\n",
       "   (u'hdfs://ip-10-111-177-131:9000/fuzzy/malejdbenmjdbenm.20.jpg',\n",
       "    u'hdfs://ip-10-111-177-131:9000/imgs/malestaffvoudcxvoudcx.7.jpg')]),\n",
       " (4095654347513307122,\n",
       "  [(u'hdfs://ip-10-111-177-131:9000/fuzzy/malecshubbcshubb.10.jpg',\n",
       "    u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmoorsmoors.14.jpg')]),\n",
       " (4911524172607922252,\n",
       "  [(u'hdfs://ip-10-111-177-131:9000/fuzzy/femaleklclarklclar.10.jpg',\n",
       "    u'hdfs://ip-10-111-177-131:9000/imgs/malestaffhensmhensm.12.jpg'),\n",
       "   (u'hdfs://ip-10-111-177-131:9000/fuzzy/malejdbenmjdbenm.20.jpg',\n",
       "    u'hdfs://ip-10-111-177-131:9000/imgs/malestaffhensmhensm.12.jpg')]),\n",
       " (1267308497549489110,\n",
       "  [(u'hdfs://ip-10-111-177-131:9000/fuzzy/malegpapazgpapaz.10.jpg',\n",
       "    u'hdfs://ip-10-111-177-131:9000/imgs/malestaffdorajdoraj.13.jpg'),\n",
       "   (u'hdfs://ip-10-111-177-131:9000/fuzzy/malegpapazgpapaz.10.jpg',\n",
       "    u'hdfs://ip-10-111-177-131:9000/imgs/malestaffieorfieorf.2.jpg'),\n",
       "   (u'hdfs://ip-10-111-177-131:9000/fuzzy/malegpapazgpapaz.10.jpg',\n",
       "    u'hdfs://ip-10-111-177-131:9000/imgs/malestaffieorfieorf.2.jpg'),\n",
       "   (u'hdfs://ip-10-111-177-131:9000/fuzzy/malegpapazgpapaz.10.jpg',\n",
       "    u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmichaelmichael.18.jpg'),\n",
       "   (u'hdfs://ip-10-111-177-131:9000/fuzzy/malestaffmaccimacci.20.jpg',\n",
       "    u'hdfs://ip-10-111-177-131:9000/imgs/malestaffdorajdoraj.13.jpg'),\n",
       "   (u'hdfs://ip-10-111-177-131:9000/fuzzy/malestaffmaccimacci.20.jpg',\n",
       "    u'hdfs://ip-10-111-177-131:9000/imgs/malestaffieorfieorf.2.jpg'),\n",
       "   (u'hdfs://ip-10-111-177-131:9000/fuzzy/malestaffmaccimacci.20.jpg',\n",
       "    u'hdfs://ip-10-111-177-131:9000/imgs/malestaffieorfieorf.2.jpg'),\n",
       "   (u'hdfs://ip-10-111-177-131:9000/fuzzy/malestaffmaccimacci.20.jpg',\n",
       "    u'hdfs://ip-10-111-177-131:9000/imgs/malestaffmichaelmichael.18.jpg')])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined.groupByKey().map(lambda (key, value): (key, list(value))).take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore_Spark_Results.ipynb in this directory goes further into the image example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
